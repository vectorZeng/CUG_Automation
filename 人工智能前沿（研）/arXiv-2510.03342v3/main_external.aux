\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{plainnat}
\citation{team2025gemini}
\citation{comanici2025gemini}
\citation{zitkovich2023rt,intelligence2025pi05,bjorck2025gr00t,wen2025dexvla}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\citation{li2023interactive,chen2024spatialvlm,chen2024automating,zhi2025closed}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Gemini Robotics 1.5 family of models consists of Gemini Robotics 1.5, a VLA, and Gemini Robotics 1.5-ER, a VLM with state-of-the-art embodied reasoning capabilities. They can be combined together to form a powerful agentic framework.}}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gr1.5_overview}{{1}{2}{The Gemini Robotics 1.5 family of models consists of Gemini Robotics 1.5, a VLA, and Gemini Robotics 1.5-ER, a VLM with state-of-the-art embodied reasoning capabilities. They can be combined together to form a powerful agentic framework}{figure.caption.1}{}}
\newlabel{fig:gr1.5_overview@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method Overview}{2}{section.2}\protected@file@percent }
\newlabel{sec:overview}{{2}{2}{Method Overview}{section.2}{}}
\newlabel{sec:overview@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model \& Architecture}{2}{subsection.2.1}\protected@file@percent }
\citation{lee2025molmoact,lin2025onetwovla,huang2025thinkact,Zawalski24-ecot}
\citation{smith2025steer,belkhale2024rth}
\citation{aloha2}
\citation{fr3}
\citation{apollo}
\citation{TodorovET12}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces GR\ 1.5{} can control three different robots with the same checkpoint to accomplish a variety of tasks out-of-the-box.}}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:robot_rollout_examples}{{2}{4}{\grshortlatest {} can control three different robots with the same checkpoint to accomplish a variety of tasks out-of-the-box}{figure.caption.2}{}}
\newlabel{fig:robot_rollout_examples@cref}{{[figure][2][]2}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Robot Data}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Evaluation}{4}{subsection.2.3}\protected@file@percent }
\citation{team2025gemini}
\citation{team2025gemini}
\citation{team2025gemini}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gemini Robotics\ 1.5{} is a general multi-embodiment Vision-Language-Action Model}{5}{section.3}\protected@file@percent }
\newlabel{sec:results-actions}{{3}{5}{\grlatest {} is a general multi-embodiment Vision-Language-Action Model}{section.3}{}}
\newlabel{sec:results-actions@cref}{{[section][3][]3}{[1][4][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gemini Robotics\ 1.5{} can generalize to new environments and tasks}{5}{subsection.3.1}\protected@file@percent }
\newlabel{sec:generalization}{{3.1}{5}{\grlatest {} can generalize to new environments and tasks}{subsection.3.1}{}}
\newlabel{sec:generalization@cref}{{[subsection][1][3]3.1}{[1][5][]5}}
\newlabel{fig:aloha-progress-generalization}{{\caption@xref {fig:aloha-progress-generalization}{ on input line 28}}{6}{\grlatest {} is a general multi-embodiment Vision-Language-Action Model}{figure.caption.3}{}}
\newlabel{fig:aloha-progress-generalization@cref}{{[section][3][]3}{[1][5][]6}}
\newlabel{sub@fig:aloha-progress-generalization}{{}{6}{\grlatest {} is a general multi-embodiment Vision-Language-Action Model}{figure.caption.3}{}}
\newlabel{sub@fig:aloha-progress-generalization@cref}{{[section][3][]3}{[1][5][]6}}
\newlabel{fig:omega-progress--generalization}{{\caption@xref {fig:omega-progress--generalization}{ on input line 33}}{6}{\grlatest {} is a general multi-embodiment Vision-Language-Action Model}{figure.caption.3}{}}
\newlabel{fig:omega-progress--generalization@cref}{{[section][3][]3}{[1][5][]6}}
\newlabel{sub@fig:omega-progress--generalization}{{}{6}{\grlatest {} is a general multi-embodiment Vision-Language-Action Model}{figure.caption.3}{}}
\newlabel{sub@fig:omega-progress--generalization@cref}{{[section][3][]3}{[1][5][]6}}
\newlabel{fig:atari-progress--generalization}{{\caption@xref {fig:atari-progress--generalization}{ on input line 38}}{6}{\grlatest {} is a general multi-embodiment Vision-Language-Action Model}{figure.caption.3}{}}
\newlabel{fig:atari-progress--generalization@cref}{{[section][3][]3}{[1][5][]6}}
\newlabel{sub@fig:atari-progress--generalization}{{}{6}{\grlatest {} is a general multi-embodiment Vision-Language-Action Model}{figure.caption.3}{}}
\newlabel{sub@fig:atari-progress--generalization@cref}{{[section][3][]3}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Breakdown of GR\ 1.5{} generalization capabilities across our robots. GR\ 1.5{} consistently outperforms the baselines and handles all four types of variations more effectively. }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:generalization}{{3}{6}{Breakdown of \grshortlatest {} generalization capabilities across our robots. \grshortlatest {} consistently outperforms the baselines and handles all four types of variations more effectively}{figure.caption.3}{}}
\newlabel{fig:generalization@cref}{{[figure][3][]3}{[1][5][]6}}
\citation{10611477}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Ablation on datasets and training recipes on ALOHA, Bi-arm Franka and Apollo Humanoid. GR\ 1.5{} consistently outperforms our baselines: GR\ 1.5{} trained on single or multi-robot data without the MT recipe. }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:data-training-ablation}{{4}{7}{Ablation on datasets and training recipes on ALOHA, Bi-arm Franka and Apollo Humanoid. \grshortlatest {} consistently outperforms our baselines: \grshortlatest {} trained on single or multi-robot data without the MT recipe}{figure.caption.4}{}}
\newlabel{fig:data-training-ablation@cref}{{[figure][4][]4}{[1][5][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Cross embodiment benchmark. Left: Our model shows zero-shot skill transfer on tasks only seen by another robot embodiment. Right: Example tasks trained on the first embodiment and evaluated on the second.}}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:cross_data_source_transfer}{{5}{8}{Cross embodiment benchmark. Left: Our model shows zero-shot skill transfer on tasks only seen by another robot embodiment. Right: Example tasks trained on the first embodiment and evaluated on the second}{figure.caption.5}{}}
\newlabel{fig:cross_data_source_transfer@cref}{{[figure][5][]5}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Learning across different robot embodiments}{8}{subsection.3.2}\protected@file@percent }
\newlabel{sec:positive_transfer}{{3.2}{8}{Learning across different robot embodiments}{subsection.3.2}{}}
\newlabel{sec:positive_transfer@cref}{{[subsection][2][3]3.2}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Thinking Helps Acting}{9}{subsection.3.3}\protected@file@percent }
\newlabel{sec:thinking}{{3.3}{9}{Thinking Helps Acting}{subsection.3.3}{}}
\newlabel{sec:thinking@cref}{{[subsection][3][3]3.3}{[1][9][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Task progress in the multi-step benchmark with and without enabling thinking during inference.}}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:medium-level-thinking}{{6}{9}{Task progress in the multi-step benchmark with and without enabling thinking during inference}{figure.caption.6}{}}
\newlabel{fig:medium-level-thinking@cref}{{[figure][6][]6}{[1][9][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces From left to right, top to bottom: an example rollout of the Thinking VLA: the Apollo humanoid packing objects into a white bag. The thinking trace are overlaid on each snapshot. The Thinking VLA is able to think about its actions at different levels, allowing it to accomplish tasks requiring semantic reasoning and multiple steps of execution.}}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:thinking-traces-visualization}{{7}{10}{From left to right, top to bottom: an example rollout of the Thinking VLA: the Apollo humanoid packing objects into a white bag. The thinking trace are overlaid on each snapshot. The Thinking VLA is able to think about its actions at different levels, allowing it to accomplish tasks requiring semantic reasoning and multiple steps of execution}{figure.caption.7}{}}
\newlabel{fig:thinking-traces-visualization@cref}{{[figure][7][]7}{[1][9][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}{Gemini Robotics-ER 1.5{}} is a generalist embodied reasoning model}{10}{section.4}\protected@file@percent }
\newlabel{sec:results-er}{{4}{10}{{\grlatestER {}} is a generalist embodied reasoning model}{section.4}{}}
\newlabel{sec:results-er@cref}{{[section][4][]4}{[1][10][]10}}
\citation{fu2024blink}
\citation{tong2024cambrian1}
\citation{team2025gemini}
\citation{song2025robospatial}
\citation{cheng2025pointarena}
\citation{yuan2024robopoint}
\citation{zhou2025roborefer}
\citation{mmmu}
\citation{rein2024gpqa}
\citation{Gauthier2024aider}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The Gemini Robotics-ER 1.5{} model is our most advanced model for embodied reasoning while retaining strong performance as a general-purpose multimodal foundation model. We measure Embodied Reasoning performance on a mix of academic benchmarks covering text-based image understanding as well as spatial signal prediction, and measure Generality performance on MMMU, GPQA, and Aider Polyglot.}}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:gr1.5_er_generality}{{8}{11}{The \grlatestER {} model is our most advanced model for embodied reasoning while retaining strong performance as a general-purpose multimodal foundation model. We measure Embodied Reasoning performance on a mix of academic benchmarks covering text-based image understanding as well as spatial signal prediction, and measure Generality performance on MMMU, GPQA, and Aider Polyglot}{figure.caption.8}{}}
\newlabel{fig:gr1.5_er_generality@cref}{{[figure][8][]8}{[1][10][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Generality}{11}{subsection.4.1}\protected@file@percent }
\newlabel{sec:results-er-generality}{{4.1}{11}{Generality}{subsection.4.1}{}}
\newlabel{sec:results-er-generality@cref}{{[subsection][1][4]4.1}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The Gemini Robotics-ER 1.5{} model has a diverse set of capabilities that can be applied on images and videos that are useful for robotics.}}{12}{figure.caption.9}\protected@file@percent }
\newlabel{fig:gr1.5_er_capability_all}{{9}{12}{The \grlatestER {} model has a diverse set of capabilities that can be applied on images and videos that are useful for robotics}{figure.caption.9}{}}
\newlabel{fig:gr1.5_er_capability_all@cref}{{[figure][9][]9}{[1][10][]12}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Performance on a mix of 5 academic benchmarks for 2D pointing and point-based reasoning; accuracy is defined as the percentage of point predictions within the ground truth mask (pointing) or correct final count (point-to-count). The categories describe different types of point prediction and are used to aggregate evaluation results from Point-Bench, RefSpatial, RoboSpatial, Where2Place, and PixMo Count. Results for GPT-5 and GPT-5-mini obtained via API calls in September 2025.}}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:gr1.5_er_pointing_benchmark}{{10}{12}{Performance on a mix of 5 academic benchmarks for 2D pointing and point-based reasoning; accuracy is defined as the percentage of point predictions within the ground truth mask (pointing) or correct final count (point-to-count). The categories describe different types of point prediction and are used to aggregate evaluation results from Point-Bench, RefSpatial, RoboSpatial, Where2Place, and PixMo Count. Results for GPT-5 and GPT-5-mini obtained via API calls in September 2025}{figure.caption.10}{}}
\newlabel{fig:gr1.5_er_pointing_benchmark@cref}{{[figure][10][]10}{[1][11][]12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Frontier capabilities for Embodied Reasoning}{12}{subsection.4.2}\protected@file@percent }
\citation{du2023vision,rocamonde2023vision}
\citation{ma2024generative}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Complex pointing examples from GR-ER 1.5{}. The model can follow complex pointing prompts that require reasoning about physical, spatial, and semantic constraints: It can localize precise parts of objects, such as the rim of a bowl and sockets of a power strip (Row 2, Columns 1 and 3) and predict points that respect physical, spatial, and semantic constraints, e.g., corresponding to objects that are lighter than $10$ pounds (Row 1, Column 3), and matching similar items (Row 2, Column 2). GR-ER 1.5{} can also sequence points into trajectories that respect physics (Row 1, Column 2) and avoid collisions (Row 1, Column 1).}}{13}{figure.caption.11}\protected@file@percent }
\newlabel{fig:gr1.5_er_pointing_collage}{{11}{13}{Complex pointing examples from \grshortlatestER {}. The model can follow complex pointing prompts that require reasoning about physical, spatial, and semantic constraints: It can localize precise parts of objects, such as the rim of a bowl and sockets of a power strip (Row 2, Columns 1 and 3) and predict points that respect physical, spatial, and semantic constraints, e.g., corresponding to objects that are lighter than $10$ pounds (Row 1, Column 3), and matching similar items (Row 2, Column 2). \grshortlatestER {} can also sequence points into trajectories that respect physics (Row 1, Column 2) and avoid collisions (Row 1, Column 1)}{figure.caption.11}{}}
\newlabel{fig:gr1.5_er_pointing_collage@cref}{{[figure][11][]11}{[1][11][]13}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Multiple forms of progress understanding in GR-ER 1.5{}. Understanding progress in scenes with robot interaction requires spatial, temporal, and semantic reasoning abilities potentially across multiple viewpoints and conditioned on language descriptions. Top: Predicting the percentage of task completion. Bottom left: Multi-view success detection: no single camera has sufficient information to detect success for the task ``put the orange pack of biscuits from the basket in the shelf next to the blue bowl.'' Bottom right: unshuffling video frames is another form of progress understanding where temporal understanding is essential.}}{14}{figure.caption.12}\protected@file@percent }
\newlabel{fig:gr1.5_er_sd_collage}{{12}{14}{Multiple forms of progress understanding in \grshortlatestER {}. Understanding progress in scenes with robot interaction requires spatial, temporal, and semantic reasoning abilities potentially across multiple viewpoints and conditioned on language descriptions. Top: Predicting the percentage of task completion. Bottom left: Multi-view success detection: no single camera has sufficient information to detect success for the task ``put the orange pack of biscuits from the basket in the shelf next to the blue bowl.'' Bottom right: unshuffling video frames is another form of progress understanding where temporal understanding is essential}{figure.caption.12}{}}
\newlabel{fig:gr1.5_er_sd_collage@cref}{{[figure][12][]12}{[1][13][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Performance on various formulations of success detection (SD). Real-time SD considers model inference latency when computing prediction accuracy, while offline success detection assumes unlimited inference time for each prediction. Multiview SD uses multiple camera views while Singleview SD uses just a single viewpoint. }}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig:gr1.5_er_sd_results}{{13}{14}{Performance on various formulations of success detection (SD). Real-time SD considers model inference latency when computing prediction accuracy, while offline success detection assumes unlimited inference time for each prediction. Multiview SD uses multiple camera views while Singleview SD uses just a single viewpoint}{figure.caption.13}{}}
\newlabel{fig:gr1.5_er_sd_results@cref}{{[figure][13][]13}{[1][13][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces (a) Example real-world use case in an inspection task. Using Gemini Robotics-ER 1.5{}, we can parse an image of an inventory shelf (left) into a table and present the result in an HTML page. (b) Performance on data distributions sourced from real-world use cases from early testers of Gemini Robotics-ER. Scores are measured as IOU for bounding box predictions and accuracy of predicted points within ground truth segmentation masks for pointing. Results for GPT-5 and GPT-5-mini obtained via API calls in September 2025.}}{15}{figure.caption.14}\protected@file@percent }
\newlabel{fig:gr1.5_er_ttp_benchmark_visual_overall}{{14}{15}{(a) Example real-world use case in an inspection task. Using \grlatestER {}, we can parse an image of an inventory shelf (left) into a table and present the result in an HTML page. (b) Performance on data distributions sourced from real-world use cases from early testers of Gemini Robotics-ER. Scores are measured as IOU for bounding box predictions and accuracy of predicted points within ground truth segmentation masks for pointing. Results for GPT-5 and GPT-5-mini obtained via API calls in September 2025}{figure.caption.14}{}}
\newlabel{fig:gr1.5_er_ttp_benchmark_visual_overall@cref}{{[figure][14][]14}{[1][15][]15}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Sample thinking traces from GR-ER 1.5{} performing embodied reasoning tasks.}}{16}{figure.caption.15}\protected@file@percent }
\newlabel{fig:gr1.5_er_sample_thoughts}{{15}{16}{Sample thinking traces from \grshortlatestER {} performing embodied reasoning tasks}{figure.caption.15}{}}
\newlabel{fig:gr1.5_er_sample_thoughts@cref}{{[figure][15][]15}{[1][16][]16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Thinking}{16}{subsection.4.3}\protected@file@percent }
\citation{2022_palm_saycan,huang2022inner,shi2025hi}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces (Left) GR-ER 1.5{} uses inference-time compute to improve performance. (Center) GR-ER 1.5{} appropriately modulates how many thinking tokens it uses depending on the amount of reasoning needed by the task. Given the same thinking budget, GR-ER 1.5{} uses fewest tokens for pointing tasks, and most for video QA. (Right) GR-ER 1.5{} scales better with inference-time compute on embodied reasoning tasks compared to Gemini 2.5 Flash. All data points are the average of 3 evaluation runs over the same benchmark sets.}}{17}{figure.caption.16}\protected@file@percent }
\newlabel{fig:gr1.5_er_thinking_scaling}{{16}{17}{(Left) \grshortlatestER {} uses inference-time compute to improve performance. (Center) \grshortlatestER {} appropriately modulates how many thinking tokens it uses depending on the amount of reasoning needed by the task. Given the same thinking budget, \grshortlatestER {} uses fewest tokens for pointing tasks, and most for video QA. (Right) \grshortlatestER {} scales better with inference-time compute on embodied reasoning tasks compared to Gemini 2.5 Flash. All data points are the average of 3 evaluation runs over the same benchmark sets}{figure.caption.16}{}}
\newlabel{fig:gr1.5_er_thinking_scaling@cref}{{[figure][16][]16}{[1][16][]17}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Gemini Robotics\ 1.5{}: A Physical Agent}{17}{section.5}\protected@file@percent }
\newlabel{sec:results-agentic}{{5}{17}{\grlatest {}: A Physical Agent}{section.5}{}}
\newlabel{sec:results-agentic@cref}{{[section][5][]5}{[1][17][]17}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Long-horizon evaluations for the GR\ 1.5{} Agent and the baselines on ALOHA (top) and Bi-arm Franka (bottom), consisting of tasks that require advanced real-world understanding, tool use, long-horizon task planning, execution, and error recovery to successfully complete the complex long-horizon tasks.}}{18}{figure.caption.17}\protected@file@percent }
\newlabel{fig:agentic_comparison}{{17}{18}{Long-horizon evaluations for the \grshortlatest {} Agent and the baselines on ALOHA (top) and Bi-arm Franka (bottom), consisting of tasks that require advanced real-world understanding, tool use, long-horizon task planning, execution, and error recovery to successfully complete the complex long-horizon tasks}{figure.caption.17}{}}
\newlabel{fig:agentic_comparison@cref}{{[figure][17][]17}{[1][17][]18}}
\citation{ISO_TS_15066_2016,ISO10218-1:2025}
\citation{comanici2025gemini}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Failure modes for long-horizon evaluations: A planning failure is when the orchestrator makes a wrong plan or issues a wrong instruction to the VLA. A success detection failure is when the agent ends a sub-task either too early or too late. An action failure is when the VLA does not successfully complete the sub-task. }}{19}{table.caption.18}\protected@file@percent }
\newlabel{table:agent-failure-modes}{{1}{19}{Failure modes for long-horizon evaluations: A planning failure is when the orchestrator makes a wrong plan or issues a wrong instruction to the VLA. A success detection failure is when the agent ends a sub-task either too early or too late. An action failure is when the VLA does not successfully complete the sub-task}{table.caption.18}{}}
\newlabel{table:agent-failure-modes@cref}{{[table][1][]1}{[1][19][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Responsible Development and Safety}{19}{section.6}\protected@file@percent }
\newlabel{sec:safety}{{6}{19}{Responsible Development and Safety}{section.6}{}}
\newlabel{sec:safety@cref}{{[section][6][]6}{[1][19][]19}}
\citation{sermanet2025generating}
\citation{neiss}
\citation{safe-vlms}
\citation{comanici2025gemini}
\newlabel{fig:examples_injury}{{18a}{21}{{\bf \texttt {ASIMOV-2.0-Injury}}: Do models understand physical risks and associated severity in given scenarios (text); and safety consequences of actions?}{figure.caption.19}{}}
\newlabel{fig:examples_injury@cref}{{[subfigure][1][18]18a}{[1][20][]21}}
\newlabel{sub@fig:examples_injury}{{a}{21}{{\bf \texttt {ASIMOV-2.0-Injury}}: Do models understand physical risks and associated severity in given scenarios (text); and safety consequences of actions?}{figure.caption.19}{}}
\newlabel{sub@fig:examples_injury@cref}{{[subfigure][1][18]18a}{[1][20][]21}}
\newlabel{fig:examples}{{\caption@xref {fig:examples}{ on input line 43}}{21}{Responsible Development and Safety}{figure.caption.19}{}}
\newlabel{fig:examples@cref}{{[section][6][]6}{[1][20][]21}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces {\bf  \texttt  {ASIMOV-2.0}} Physical Safety Benchmark: Instances and Key Questions}}{21}{figure.caption.19}\protected@file@percent }
\newlabel{fig:asimov2}{{18}{21}{{\bf \texttt {ASIMOV-2.0}} Physical Safety Benchmark: Instances and Key Questions}{figure.caption.19}{}}
\newlabel{fig:asimov2@cref}{{[figure][18][]18}{[1][20][]21}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces ASIMOV-2.0 Safety Evaluations.}}{21}{figure.caption.20}\protected@file@percent }
\newlabel{fig:safety_evals}{{19}{21}{ASIMOV-2.0 Safety Evaluations}{figure.caption.20}{}}
\newlabel{fig:safety_evals@cref}{{[figure][19][]19}{[1][20][]21}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Auto-Red-Teaming detects ER Hallucinations under adversarial prompts.}}{22}{figure.caption.21}\protected@file@percent }
\newlabel{fig:red-teaming}{{20}{22}{Auto-Red-Teaming detects ER Hallucinations under adversarial prompts}{figure.caption.21}{}}
\newlabel{fig:red-teaming@cref}{{[figure][20][]20}{[1][20][]22}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{22}{section.7}\protected@file@percent }
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {8}Contributions and Acknowledgments}{24}{section.8}\protected@file@percent }
\citation{mitchell2019model}
\citation{dwibedi2024flexcap}
\citation{geminiteam2023gemini}
\citation{jax2018github}
\citation{2021pathwaysarchitecture}
\citation{geminiteam2023gemini}
\gdef \LT@i {\LT@entry 
    {1}{125.35841pt}\LT@entry 
    {1}{342.61972pt}}
\newlabel{sec:appendix}{{8}{27}{}{section.8}{}}
\newlabel{sec:appendix@cref}{{[section][8][]8}{[1][27][]27}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Model Card}{27}{appendix.A}\protected@file@percent }
\newlabel{sec:model-card}{{A}{27}{Model Card}{appendix.A}{}}
\newlabel{sec:model-card@cref}{{[appendix][1][2147483647]A}{[1][27][]27}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Gemini Robotics\ 1.5{} model card.}}{28}{table.2}\protected@file@percent }
\newlabel{tab:model-card}{{2}{28}{Model Card}{table.2}{}}
\newlabel{tab:model-card@cref}{{[table][2][2147483647]2}{[1][27][]28}}
\citation{team2025gemini}
\citation{team2025gemini}
\citation{team2025gemini}
\citation{team2025gemini}
\citation{team2025gemini}
\@writefile{toc}{\contentsline {section}{\numberline {B}Gemini Robotics\ 1.5{} is a general multi-embodiment Vision-Language-Action Model}{29}{appendix.B}\protected@file@percent }
\newlabel{appendix-pre-action}{{B}{29}{\grlatest {} is a general multi-embodiment Vision-Language-Action Model}{appendix.B}{}}
\newlabel{appendix-pre-action@cref}{{[appendix][2][2147483647]B}{[1][29][]29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Rank consistency between evaluations in simulation and on real robots}{29}{subsection.B.1}\protected@file@percent }
\newlabel{sec:sim-to-real-appendix}{{B.1}{29}{Rank consistency between evaluations in simulation and on real robots}{subsection.B.1}{}}
\newlabel{sec:sim-to-real-appendix@cref}{{[subappendix][1][2147483647,2]B.1}{[1][29][]29}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Each colored pair represents an A/B test both in simulation and real. Across a range of tasks, we find that success rates are rank consistent between simulation and real. This consistency allows for rapid iteration of model architecture, training objectives, and experiment design.}}{29}{figure.caption.22}\protected@file@percent }
\newlabel{fig:sim_to_real_correlation}{{21}{29}{Each colored pair represents an A/B test both in simulation and real. Across a range of tasks, we find that success rates are rank consistent between simulation and real. This consistency allows for rapid iteration of model architecture, training objectives, and experiment design}{figure.caption.22}{}}
\newlabel{fig:sim_to_real_correlation@cref}{{[figure][21][2147483647]21}{[1][29][]29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Generalization benchmark}{29}{subsection.B.2}\protected@file@percent }
\newlabel{appendix:generalization-tasks}{{B.2}{29}{Generalization benchmark}{subsection.B.2}{}}
\newlabel{appendix:generalization-tasks@cref}{{[subappendix][2][2147483647,2]B.2}{[1][29][]29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2.1}ALOHA robot}{29}{subsubsection.B.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {B.2.1.1}New action generalization progress score definition}{29}{paragraph.B.2.1.1}\protected@file@percent }
\newlabel{appendix:new-action-gen-aloha}{{B.2.1.1}{29}{New action generalization progress score definition}{paragraph.B.2.1.1}{}}
\newlabel{appendix:new-action-gen-aloha@cref}{{[paragraph][1][2147483647,2,2,1]B.2.1.1}{[1][29][]29}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Examples of the expanded action generalization benchmark. }}{30}{figure.caption.23}\protected@file@percent }
\newlabel{fig:action-gen-tasks}{{22}{30}{Examples of the expanded action generalization benchmark}{figure.caption.23}{}}
\newlabel{fig:action-gen-tasks@cref}{{[figure][22][2147483647]22}{[1][29][]30}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Progress Scores: New action generalization tasks progress score.}}{30}{table.caption.24}\protected@file@percent }
\newlabel{tab:aloha_action_gen}{{3}{30}{Progress Scores: New action generalization tasks progress score}{table.caption.24}{}}
\newlabel{tab:aloha_action_gen@cref}{{[table][3][2147483647]3}{[1][30][]30}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {B.2.1.2}Task generalization progress score definition}{30}{paragraph.B.2.1.2}\protected@file@percent }
\newlabel{appendix:task-gen-aloha}{{B.2.1.2}{30}{Task generalization progress score definition}{paragraph.B.2.1.2}{}}
\newlabel{appendix:task-gen-aloha@cref}{{[paragraph][2][2147483647,2,2,1]B.2.1.2}{[1][30][]30}}
\citation{team2025gemini}
\citation{nistassemblyboards}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Examples of task execution for the Task generalization analysis of ~\ref {sec:generalization}. }}{31}{figure.caption.25}\protected@file@percent }
\newlabel{fig:task-gen-tasks}{{23}{31}{Examples of task execution for the Task generalization analysis of ~\ref {sec:generalization}}{figure.caption.25}{}}
\newlabel{fig:task-gen-tasks@cref}{{[figure][23][2147483647]23}{[1][30][]31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2.2}Bi-arm Franka robot}{31}{subsubsection.B.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Progress Scores: Task generalization tasks.}}{32}{table.caption.26}\protected@file@percent }
\newlabel{tab:aloha_task_gen}{{4}{32}{Progress Scores: Task generalization tasks}{table.caption.26}{}}
\newlabel{tab:aloha_task_gen@cref}{{[table][4][2147483647]4}{[1][30][]32}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Scenes used to define the generalization benchmark for Bi-Arm Franka robot. Left: A workbench inspired scene that allows for manipulation of tools and skills ranging from hanging, unhanging, picking and placing. Center: a scene with a computer and assorted cables and peripherals, allowing for cable handling, insertions and removals. Left: Layout of the National Institute of Science and Technology (NIST) Task Board 2 ~\citep  {nistassemblyboards}. }}{32}{figure.caption.27}\protected@file@percent }
\newlabel{fig:omega-scenes}{{24}{32}{Scenes used to define the generalization benchmark for Bi-Arm Franka robot. Left: A workbench inspired scene that allows for manipulation of tools and skills ranging from hanging, unhanging, picking and placing. Center: a scene with a computer and assorted cables and peripherals, allowing for cable handling, insertions and removals. Left: Layout of the National Institute of Science and Technology (NIST) Task Board 2 ~\cite {nistassemblyboards}}{figure.caption.27}{}}
\newlabel{fig:omega-scenes@cref}{{[figure][24][2147483647]24}{[1][31][]32}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Example variations of scenes used for measuring performance across generalization axes on the Bi-arm Franka platform.}}{33}{figure.caption.28}\protected@file@percent }
\newlabel{fig:example_of_gen_omega}{{25}{33}{Example variations of scenes used for measuring performance across generalization axes on the Bi-arm Franka platform}{figure.caption.28}{}}
\newlabel{fig:example_of_gen_omega@cref}{{[figure][25][2147483647]25}{[1][33][]33}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {B.2.2.1}Task progress score for in-distribution, visual and action generalization tasks}{33}{paragraph.B.2.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Progress Scores: Bi-arm Franka (Workbench scene).}}{34}{table.caption.29}\protected@file@percent }
\newlabel{tab:franka_workbench}{{5}{34}{Progress Scores: Bi-arm Franka (Workbench scene)}{table.caption.29}{}}
\newlabel{tab:franka_workbench@cref}{{[table][5][2147483647]5}{[1][33][]34}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Progress Scores: Bi-arm Franka (Computer scene).}}{35}{table.caption.30}\protected@file@percent }
\newlabel{tab:franka_computer}{{6}{35}{Progress Scores: Bi-arm Franka (Computer scene)}{table.caption.30}{}}
\newlabel{tab:franka_computer@cref}{{[table][6][2147483647]6}{[1][34][]35}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {B.2.2.2}Task progress score for semantic generalization tasks}{35}{paragraph.B.2.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Progress Scores: Bi-arm Franka (NIST Assembly Task Board 2).}}{36}{table.caption.31}\protected@file@percent }
\newlabel{tab:franka_nist}{{7}{36}{Progress Scores: Bi-arm Franka (NIST Assembly Task Board 2)}{table.caption.31}{}}
\newlabel{tab:franka_nist@cref}{{[table][7][2147483647]7}{[1][35][]36}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Progress Scores: Bi-arm Franka (Semantic Gen. - Workbench).}}{36}{table.caption.32}\protected@file@percent }
\newlabel{tab:franka_semantic_gen_workbench}{{8}{36}{Progress Scores: Bi-arm Franka (Semantic Gen. - Workbench)}{table.caption.32}{}}
\newlabel{tab:franka_semantic_gen_workbench@cref}{{[table][8][2147483647]8}{[1][35][]36}}
\citation{team2025gemini}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Progress Scores: Bi-arm Franka (Semantic Gen. - Computer Scene).}}{37}{table.caption.33}\protected@file@percent }
\newlabel{tab:franka_semantic_gen_computer}{{9}{37}{Progress Scores: Bi-arm Franka (Semantic Gen. - Computer Scene)}{table.caption.33}{}}
\newlabel{tab:franka_semantic_gen_computer@cref}{{[table][9][2147483647]9}{[1][36][]37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2.3}Apollo humanoid robot}{37}{subsubsection.B.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Example variations of scenes used for measuring performance across generalization axes on the Apollo humanoid platform. }}{37}{figure.caption.34}\protected@file@percent }
\newlabel{fig:example_of_gen_atari}{{26}{37}{Example variations of scenes used for measuring performance across generalization axes on the Apollo humanoid platform}{figure.caption.34}{}}
\newlabel{fig:example_of_gen_atari@cref}{{[figure][26][2147483647]26}{[1][37][]37}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {B.2.3.1}Task progress score for in-distribution, visual, semantic and action generalization tasks}{37}{paragraph.B.2.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Progress Scores: Apollo humanoid (In-distribution and Visual Gen.).}}{38}{table.caption.35}\protected@file@percent }
\newlabel{tab:apollo_visual_gen}{{10}{38}{Progress Scores: Apollo humanoid (In-distribution and Visual Gen.)}{table.caption.35}{}}
\newlabel{tab:apollo_visual_gen@cref}{{[table][10][2147483647]10}{[1][37][]38}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Progress Scores: Apollo humanoid (Semantic Generalization).}}{38}{table.caption.36}\protected@file@percent }
\newlabel{tab:apollo_semantic_gen}{{11}{38}{Progress Scores: Apollo humanoid (Semantic Generalization)}{table.caption.36}{}}
\newlabel{tab:apollo_semantic_gen@cref}{{[table][11][2147483647]11}{[1][37][]38}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Progress Scores: Apollo humanoid (Action Generalization).}}{39}{table.caption.37}\protected@file@percent }
\newlabel{tab:apollo_action_gen}{{12}{39}{Progress Scores: Apollo humanoid (Action Generalization)}{table.caption.37}{}}
\newlabel{tab:apollo_action_gen@cref}{{[table][12][2147483647]12}{[1][37][]39}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {B.2.3.2}Qualitative Results}{40}{paragraph.B.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Qualitative examples of generalization on the humanoid robot: Gemini Robotics\ 1.5{} learns multiple grasp strategies, can manipulate objects at different heights, and grasp novel objects and place them into receptacles that were never seen during training. }}{40}{figure.caption.38}\protected@file@percent }
\newlabel{fig:humanoid_gen_images}{{27}{40}{Qualitative examples of generalization on the humanoid robot: \grlatest {} learns multiple grasp strategies, can manipulate objects at different heights, and grasp novel objects and place them into receptacles that were never seen during training}{figure.caption.38}{}}
\newlabel{fig:humanoid_gen_images@cref}{{[figure][27][2147483647]27}{[1][40][]40}}
\citation{team2025gemini}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Cross-embodiment benchmark}{41}{subsection.B.3}\protected@file@percent }
\newlabel{appendix:cross-embodiment-benchmark}{{B.3}{41}{Cross-embodiment benchmark}{subsection.B.3}{}}
\newlabel{appendix:cross-embodiment-benchmark@cref}{{[subappendix][3][2147483647,2]B.3}{[1][41][]41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3.1}Bi-arm Franka $\to $ ALOHA benchmark}{41}{subsubsection.B.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Example of execution of cross-embodiment tasks for Bi-arm Franka $\to $ ALOHA benchmark. }}{41}{figure.caption.39}\protected@file@percent }
\newlabel{fig:omega2aloha-images}{{28}{41}{Example of execution of cross-embodiment tasks for Bi-arm Franka \texorpdfstring {$\to $}{->} ALOHA benchmark}{figure.caption.39}{}}
\newlabel{fig:omega2aloha-images@cref}{{[figure][28][2147483647]28}{[1][41][]41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3.2}ALOHA benchmark $\to $ Bi-arm Franka and Apollo humanoid robot $\to $ Bi-arm Franka}{41}{subsubsection.B.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Example of execution of cross-embodiment tasks for ALOHA $\to $ Bi-arm Franka benchmark. }}{42}{figure.caption.41}\protected@file@percent }
\newlabel{fig:aloha2omega-images}{{29}{42}{Example of execution of cross-embodiment tasks for ALOHA \texorpdfstring {$\to $}{->} Bi-arm Franka benchmark}{figure.caption.41}{}}
\newlabel{fig:aloha2omega-images@cref}{{[figure][29][2147483647]29}{[1][42][]42}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Cross-embodiment tasks for Apollo humanoid $\to $ Bi-arm Franka benchmark. }}{42}{figure.caption.42}\protected@file@percent }
\newlabel{fig:atari2omega-images}{{30}{42}{Cross-embodiment tasks for Apollo humanoid $\to $ Bi-arm Franka benchmark}{figure.caption.42}{}}
\newlabel{fig:atari2omega-images@cref}{{[figure][30][2147483647]30}{[1][42][]42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3.3}ALOHA benchmark $\to $ Humanoid robot}{42}{subsubsection.B.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Cross-embodiment tasks for ALOHA $\to $ Humanoid benchmark. }}{43}{figure.caption.44}\protected@file@percent }
\newlabel{fig:atari-cross-images}{{31}{43}{Cross-embodiment tasks for ALOHA \texorpdfstring {$\to $}{->} Humanoid benchmark}{figure.caption.44}{}}
\newlabel{fig:atari-cross-images@cref}{{[figure][31][2147483647]31}{[1][42][]43}}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Progress Scores: Bi-arm Franka $\to $ ALOHA Benchmark.}}{44}{table.caption.40}\protected@file@percent }
\newlabel{tab:franka_to_aloha_benchmark}{{13}{44}{Progress Scores: Bi-arm Franka \texorpdfstring {$\to $}{->} ALOHA Benchmark}{table.caption.40}{}}
\newlabel{tab:franka_to_aloha_benchmark@cref}{{[table][13][2147483647]13}{[1][41][]44}}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Progress Scores: ALOHA/Humanoid $\to $ Bi-arm Franka.}}{45}{table.caption.43}\protected@file@percent }
\newlabel{tab:cross_emb_to_franka}{{14}{45}{Progress Scores: ALOHA/Humanoid \texorpdfstring {$\to $}{->} Bi-arm Franka}{table.caption.43}{}}
\newlabel{tab:cross_emb_to_franka@cref}{{[table][14][2147483647]14}{[1][42][]45}}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Progress Scores: ALOHA $\to $ Humanoid Benchmark.}}{45}{table.caption.45}\protected@file@percent }
\newlabel{tab:aloha_to_humanoid_benchmark}{{15}{45}{Progress Scores: ALOHA \texorpdfstring {$\to $}{->} Humanoid Benchmark}{table.caption.45}{}}
\newlabel{tab:aloha_to_humanoid_benchmark@cref}{{[table][15][2147483647]15}{[1][42][]45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Multi-step benchmark}{46}{subsection.B.4}\protected@file@percent }
\newlabel{appendix:medium-horizon-benchmark}{{B.4}{46}{Multi-step benchmark}{subsection.B.4}{}}
\newlabel{appendix:medium-horizon-benchmark@cref}{{[subappendix][4][2147483647,2]B.4}{[1][46][]46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.4.1}ALOHA robot}{46}{subsubsection.B.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Tasks in the multi-step benchmark for the ALOHA. }}{46}{figure.caption.46}\protected@file@percent }
\newlabel{fig:medium-horizon-aloha}{{32}{46}{Tasks in the multi-step benchmark for the ALOHA}{figure.caption.46}{}}
\newlabel{fig:medium-horizon-aloha@cref}{{[figure][32][2147483647]32}{[1][46][]46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.4.2}Bi-arm Franka robot}{46}{subsubsection.B.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.4.3}Humanoid robot}{46}{subsubsection.B.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces Progress Scores: ALOHA Robot (Multi-step benchmark).}}{47}{table.caption.47}\protected@file@percent }
\newlabel{tab:aloha_medium_horizon}{{16}{47}{Progress Scores: ALOHA Robot (Multi-step benchmark)}{table.caption.47}{}}
\newlabel{tab:aloha_medium_horizon@cref}{{[table][16][2147483647]16}{[1][46][]47}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Tasks in the multi-step benchmark for the Bi-arm Franka. }}{48}{figure.caption.48}\protected@file@percent }
\newlabel{fig:medium-horizon-omegs}{{33}{48}{Tasks in the multi-step benchmark for the Bi-arm Franka}{figure.caption.48}{}}
\newlabel{fig:medium-horizon-omegs@cref}{{[figure][33][2147483647]33}{[1][46][]48}}
\@writefile{lot}{\contentsline {table}{\numberline {17}{\ignorespaces Progress Scores: Bi-arm Franka (Multi-step benchmark).}}{49}{table.caption.49}\protected@file@percent }
\newlabel{tab:franka_medium_horizon}{{17}{49}{Progress Scores: Bi-arm Franka (Multi-step benchmark)}{table.caption.49}{}}
\newlabel{tab:franka_medium_horizon@cref}{{[table][17][2147483647]17}{[1][46][]49}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Tasks in the multi-step benchmark for the Apollo humanoid. }}{50}{figure.caption.50}\protected@file@percent }
\newlabel{fig:atari-images-medium}{{34}{50}{Tasks in the multi-step benchmark for the Apollo humanoid}{figure.caption.50}{}}
\newlabel{fig:atari-images-medium@cref}{{[figure][34][2147483647]34}{[1][46][]50}}
\@writefile{lot}{\contentsline {table}{\numberline {18}{\ignorespaces Progress Scores: Humanoid Robot (Multi-step benchmark).}}{51}{table.caption.51}\protected@file@percent }
\newlabel{tab:humanoid_medium_horizon}{{18}{51}{Progress Scores: Humanoid Robot (Multi-step benchmark)}{table.caption.51}{}}
\newlabel{tab:humanoid_medium_horizon@cref}{{[table][18][2147483647]18}{[1][46][]51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Success rate}{52}{subsection.B.5}\protected@file@percent }
\newlabel{appendix:additional-exps}{{B.5}{52}{Success rate}{subsection.B.5}{}}
\newlabel{appendix:additional-exps@cref}{{[subappendix][5][2147483647,2]B.5}{[1][52][]52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.5.1}Success rate for generalization performance}{52}{subsubsection.B.5.1}\protected@file@percent }
\newlabel{fig:aloha-sr-generalization}{{\caption@xref {fig:aloha-sr-generalization}{ on input line 2012}}{52}{Success rate for generalization performance}{figure.caption.52}{}}
\newlabel{fig:aloha-sr-generalization@cref}{{[subsubappendix][1][2147483647,2,5]B.5.1}{[1][52][]52}}
\newlabel{sub@fig:aloha-sr-generalization}{{}{52}{Success rate for generalization performance}{figure.caption.52}{}}
\newlabel{sub@fig:aloha-sr-generalization@cref}{{[subsubappendix][1][2147483647,2,5]B.5.1}{[1][52][]52}}
\newlabel{fig:omega-sr-generalization}{{\caption@xref {fig:omega-sr-generalization}{ on input line 2017}}{52}{Success rate for generalization performance}{figure.caption.52}{}}
\newlabel{fig:omega-sr-generalization@cref}{{[subsubappendix][1][2147483647,2,5]B.5.1}{[1][52][]52}}
\newlabel{sub@fig:omega-sr-generalization}{{}{52}{Success rate for generalization performance}{figure.caption.52}{}}
\newlabel{sub@fig:omega-sr-generalization@cref}{{[subsubappendix][1][2147483647,2,5]B.5.1}{[1][52][]52}}
\newlabel{fig:atari-sr-generalization}{{\caption@xref {fig:atari-sr-generalization}{ on input line 2022}}{52}{Success rate for generalization performance}{figure.caption.52}{}}
\newlabel{fig:atari-sr-generalization@cref}{{[subsubappendix][1][2147483647,2,5]B.5.1}{[1][52][]52}}
\newlabel{sub@fig:atari-sr-generalization}{{}{52}{Success rate for generalization performance}{figure.caption.52}{}}
\newlabel{sub@fig:atari-sr-generalization@cref}{{[subsubappendix][1][2147483647,2,5]B.5.1}{[1][52][]52}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces  Breakdown of GR\ 1.5{} generalization capabilities across our robots. GR\ 1.5{} consistently outperforms the baselines and handles all four types of variations more effectively.}}{52}{figure.caption.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.5.2}Success rate for data and model ablation}{53}{subsubsection.B.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces  Ablation on datasets and training recipes on our robots: GR\ 1.5{} consistently outperforms our baselines: GR\ 1.5{} trained on single or multi-robot data without the MT recipe. }}{53}{figure.caption.53}\protected@file@percent }
\newlabel{fig:data-training-ablation-success}{{36}{53}{Ablation on datasets and training recipes on our robots: \grshortlatest {} consistently outperforms our baselines: \grshortlatest {} trained on single or multi-robot data without the MT recipe}{figure.caption.53}{}}
\newlabel{fig:data-training-ablation-success@cref}{{[figure][36][2147483647]36}{[1][53][]53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.5.3}Success rate for thinking ablation}{54}{subsubsection.B.5.3}\protected@file@percent }
\newlabel{sec:thinking-appendix}{{B.5.3}{54}{Success rate for thinking ablation}{subsubsection.B.5.3}{}}
\newlabel{sec:thinking-appendix@cref}{{[subsubappendix][3][2147483647,2,5]B.5.3}{[1][54][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Ablation of thinking: success rate in the multi-step benchmark with and without enabling thinking during inference.}}{54}{figure.caption.54}\protected@file@percent }
\newlabel{fig:medium-level-thinking-sr}{{37}{54}{Ablation of thinking: success rate in the multi-step benchmark with and without enabling thinking during inference}{figure.caption.54}{}}
\newlabel{fig:medium-level-thinking-sr@cref}{{[figure][37][2147483647]37}{[1][54][]54}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Gemini Robotics-ER 1.5{} is a generalist embodied reasoning model}{55}{appendix.C}\protected@file@percent }
\newlabel{appendix-pre-er}{{C}{55}{\grlatestER {} is a generalist embodied reasoning model}{appendix.C}{}}
\newlabel{appendix-pre-er@cref}{{[appendix][3][2147483647]C}{[1][55][]55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Evaluation Details: Generality}{55}{subsection.C.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {19}{\ignorespaces Model performance on a mix of 15 academic embodied reasoning benchmarks. GPT-5 and GPT-5-mini results obtained via API in September 2025.}}{55}{table.caption.55}\protected@file@percent }
\newlabel{table:er-external}{{19}{55}{Model performance on a mix of 15 academic embodied reasoning benchmarks. GPT-5 and GPT-5-mini results obtained via API in September 2025}{table.caption.55}{}}
\newlabel{table:er-external@cref}{{[table][19][2147483647]19}{[1][55][]55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Evaluation Details: Pointing}{55}{subsection.C.2}\protected@file@percent }
\newlabel{appendix-pointing}{{C.2}{55}{Evaluation Details: Pointing}{subsection.C.2}{}}
\newlabel{appendix-pointing@cref}{{[subappendix][2][2147483647,3]C.2}{[1][55][]55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Additional Examples}{55}{subsection.C.3}\protected@file@percent }
\newlabel{appendix-er-examples}{{C.3}{55}{Additional Examples}{subsection.C.3}{}}
\newlabel{appendix-er-examples@cref}{{[subappendix][3][2147483647,3]C.3}{[1][55][]55}}
\@writefile{lot}{\contentsline {table}{\numberline {20}{\ignorespaces Model performance on MMMU, GPQA and Aider Polyglot benchmarks. Results for GPT-5 and GPT-5-mini obtained via API with default thinking settings and no tool use in September 2025.}}{56}{table.caption.56}\protected@file@percent }
\newlabel{table:er-generality}{{20}{56}{Model performance on MMMU, GPQA and Aider Polyglot benchmarks. Results for GPT-5 and GPT-5-mini obtained via API with default thinking settings and no tool use in September 2025}{table.caption.56}{}}
\newlabel{table:er-generality@cref}{{[table][20][2147483647]20}{[1][55][]56}}
\@writefile{lot}{\contentsline {table}{\numberline {21}{\ignorespaces Model performance on complex pointing benchmarks, broken down by subtask.}}{56}{table.caption.57}\protected@file@percent }
\newlabel{table:pointing-breakdown}{{21}{56}{Model performance on complex pointing benchmarks, broken down by subtask}{table.caption.57}{}}
\newlabel{table:pointing-breakdown@cref}{{[table][21][2147483647]21}{[1][55][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Sample thoughts from GR-ER 1.5{} performing embodied reasoning tasks.}}{57}{figure.caption.58}\protected@file@percent }
\newlabel{fig:gr1.5_er_sample_thoughts_4}{{38}{57}{Sample thoughts from \grshortlatestER {} performing embodied reasoning tasks}{figure.caption.58}{}}
\newlabel{fig:gr1.5_er_sample_thoughts_4@cref}{{[figure][38][2147483647]38}{[1][55][]57}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Gemini Robotics\ 1.5{}: A Physical Agent}{58}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Long-horizon benchmarks}{58}{subsection.D.1}\protected@file@percent }
\newlabel{appendix:long-horizon}{{D.1}{58}{Long-horizon benchmarks}{subsection.D.1}{}}
\newlabel{appendix:long-horizon@cref}{{[subappendix][1][2147483647,4]D.1}{[1][58][]58}}
\@writefile{lot}{\contentsline {table}{\numberline {22}{\ignorespaces Progress Scores: ALOHA Robot (Long-horizon Benchmark).}}{58}{table.caption.59}\protected@file@percent }
\newlabel{tab:aloha_long_horizon}{{22}{58}{Progress Scores: ALOHA Robot (Long-horizon Benchmark)}{table.caption.59}{}}
\newlabel{tab:aloha_long_horizon@cref}{{[table][22][2147483647]22}{[1][58][]58}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces ALOHA long-horizon benchmark. }}{58}{figure.caption.60}\protected@file@percent }
\newlabel{fig:aloha-long-horizon-rollout}{{39}{58}{ALOHA long-horizon benchmark}{figure.caption.60}{}}
\newlabel{fig:aloha-long-horizon-rollout@cref}{{[figure][39][2147483647]39}{[1][58][]58}}
\@writefile{lot}{\contentsline {table}{\numberline {23}{\ignorespaces Progress Scores: Bi-arm Franka (Long-horizon Benchmark).}}{59}{table.caption.61}\protected@file@percent }
\newlabel{tab:franka_long_horizon}{{23}{59}{Progress Scores: Bi-arm Franka (Long-horizon Benchmark)}{table.caption.61}{}}
\newlabel{tab:franka_long_horizon@cref}{{[table][23][2147483647]23}{[1][58][]59}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Bi-arm Franka long-horizon benchmark. }}{59}{figure.caption.62}\protected@file@percent }
\newlabel{fig:omega-long-horizon-rollout}{{40}{59}{Bi-arm Franka long-horizon benchmark}{figure.caption.62}{}}
\newlabel{fig:omega-long-horizon-rollout@cref}{{[figure][40][2147483647]40}{[1][58][]59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Success rate}{60}{subsection.D.2}\protected@file@percent }
\newlabel{appendix:additional-exps-agentic}{{D.2}{60}{Success rate}{subsection.D.2}{}}
\newlabel{appendix:additional-exps-agentic@cref}{{[subappendix][2][2147483647,4]D.2}{[1][60][]60}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Long-horizon evaluations for the GR\ 1.5{} Agent on ALOHA (top) and Bi-arm Franka (bottom), consisting of tasks that require advanced real-world understanding, tool use, long-horizon task planning, execution and error recovery to successfully complete the complex long-horizon tasks.}}{60}{figure.caption.63}\protected@file@percent }
\newlabel{fig:agentic_sr}{{41}{60}{Long-horizon evaluations for the \grshortlatest {} Agent on ALOHA (top) and Bi-arm Franka (bottom), consisting of tasks that require advanced real-world understanding, tool use, long-horizon task planning, execution and error recovery to successfully complete the complex long-horizon tasks}{figure.caption.63}{}}
\newlabel{fig:agentic_sr@cref}{{[figure][41][2147483647]41}{[1][60][]60}}
\newlabel{LastPage}{{D.2}{60}{Success rate}{page.60}{}}
\gdef\lastpage@lastpage{60}
\gdef\lastpage@lastpageHy{60}
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{60}
