\section{Introduction}
\label{sec:intro}
 
Truly general robots will require a deep understanding of the physical world.
Our previous work, \grlegacy{}~\cite{team2025gemini}, established a strong foundation by leveraging Gemini’s rich world knowledge to create a Vision-Language-Action (VLA) model that exhibits impressive interactivity, generality, and dexterity in direct robot control. We now introduce the \textbf{\grlatest{}} (\grshortlatest{}) family of robot foundation models, built on the latest generation of Gemini~\cite{comanici2025gemini}. The new model family significantly enhances the capabilities of \grlegacy{} and brings Gemini's advanced thinking and agentic paradigm to the physical world. 
It includes \grlatest{}, a multi-embodiment VLA model~\citep{zitkovich2023rt, intelligence2025pi05, bjorck2025gr00t, wen2025dexvla} with strong reasoning and generalization, and \grlatestER{}, a generalist Vision-Language Model (VLM) that achieves a new state-of-the-art across embodied reasoning benchmarks. We combine these two models into an agentic system that enables robots to solve complex problems by orchestrating user dialogue, high-level reasoning and planning, agentic tool use and low-level action.

\noindent \textbf{\grlatestVLA{}} advances the frontier of Vision-Language-Action (VLA) pre-training by integrating two core breakthroughs. Firstly, a novel architecture and a Motion Transfer (MT) mechanism enable the model to learn from diverse robot data sources, forming a unified understanding of motion and physics. This multi-embodiment pre-training allows \grshortlatestVLA{} to control multiple robots, including the ALOHA, Bi-arm Franka, and Apollo humanoid robots, without any robot-specific post-training, and it also enables  zero-shot skill transfer from one robot to another. Secondly, \grshortlatestVLA{} is a Thinking VLA that can explicitly reason about its actions, interleaving a stream of thoughts with physical movements. This allows the model to convert visual observations into language-based thoughts, simplify complex instructions, detect task success or failure, propose recovery behaviors, and make the robot's actions more interpretable to human users.

\noindent \textbf{\grlatestER{}} (\grshortlatestER{}) advances the state-of-the-art for embodied reasoning~\citep{li2023interactive, chen2024spatialvlm, chen2024automating, zhi2025closed}, i.e.,\ the visuo-spatial-temporal understanding of the physical world that is required for robotic applications. Building upon Gemini's state-of-the-art thinking and multimodal capabilities, \grshortlatestER{} significantly outperforms other frontier models across a broad suite of embodied intelligence benchmarks, while retaining the general capabilities of a frontier model and being considerably faster. \grshortlatestER{}’s physical understanding combines naturally with Gemini’s ability to use tools, communicate using modalities like video and audio, and write code, opening up a broad spectrum of potential applications.

\noindent To achieve truly general-purpose robot agents, we combine our models in an agentic framework (Figure \ref{fig:gr1.5_overview}). This framework is key to unlocking new capabilities: it handles long-horizon task execution via complex planning and adaptive orchestration, facilitates multimodal interaction, enables robots to leverage user-specified tools (e.g. web search) to solve problems and complete tasks, and implements a multi-layered safety mechanism through explicit reasoning about safety violations. 
