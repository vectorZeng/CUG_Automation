@techreport{team2025gemini,
	title={Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer},
	author={Gemini Robotics Team},
	institution={Google DeepMind},
	year={2025}
}

@article{comanici2025gemini,
	title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
	author={Comanici, Gheorghe and others},
	journal={arXiv preprint arXiv:2507.06261},
	year={2025}
}

@inproceedings{song2025robospatial,
	title={Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics},
	author={Song, Chan Hee and others},
	booktitle={CVPR},
	year={2025}
}

@article{cheng2025pointarena,
	title={PointArena: Probing Multimodal Grounding Through Language-Guided Pointing},
	author={Cheng, Long and others},
	journal={arXiv preprint arXiv:2505.09990},
	year={2025}
}

@article{bjorck2025gr00t,
	title={Gr00t n1: An open foundation model for generalist humanoid robots},
	author={Bjorck, Johan and others},
	journal={arXiv preprint arXiv:2503.14734},
	year={2025}
}

@article{wen2025dexvla,
	title={Dexvla: Vision-language model with plug-in diffusion expert for general robot control},
	author={Wen, Junjie and others},
	journal={arXiv preprint arXiv:2502.05855},
	year={2025}
}

@article{lin2025onetwovla,
	title={OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning},
	author={Lin, Fanqi and others},
	journal={arXiv preprint arXiv:2505.11917},
	year={2025}
}

@article{huang2025thinkact,
	title={Thinkact: Vision-language-action reasoning via reinforced visual latent planning},
	author={Huang, Chi-Pin and others},
	journal={arXiv preprint arXiv:2507.16815},
	year={2025}
}

@inproceedings{zhi2025closed,
	title={Closed-loop open-vocabulary mobile manipulation with gpt-4v},
	author={Zhi, Peiyuan and others},
	booktitle={ICRA},
	year={2025}
}

@article{zitkovich2023rt,
	title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
	author={Zitkovich, Brianna and others},
	booktitle={CoRL},
	year={2023}
}

@article{du2023vision,
	title={Vision-language models as success detectors},
	author={Du, Yuqing and others},
	journal={arXiv preprint arXiv:2303.07280},
	year={2023}
}

@article{rocamonde2023vision,
	title={Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning},
	author={Rocamonde, Juan and others},
	journal={arXiv preprint arXiv:2310.12921},
	year={2023}
}

@article{li2023interactive,
	title={Interactive task planning with language models},
	author={Li, Boyi and others},
	journal={arXiv preprint arXiv:2310.10645},
	year={2023}
}

@article{chen2024automating,
	title={Automating robot failure recovery using vision-language models with optimized prompts},
	author={Chen, Hongyi and others},
	journal={arXiv preprint arXiv:2409.03966},
	year={2024}
}

@inproceedings{TodorovET12,
	title={MuJoCo: A physics engine for model-based control},
	author={Todorov, Eugene and Erez, Tom and Tassa, Yuval},
	booktitle={IROS},
	year={2012}
}

@techreport{ISO_TS_15066_2016,
	title={Robots and robotic devices -- Collaborative robots},
	author={{International Organization for Standardization}},
	institution={ISO},
	number={ISO/TS 15066:2016},
	year={2016}
}

@techreport{ISO10218-1:2025,
	title={Robotics — Safety requirements — Part 1: Industrial robots},
	author={{International Organization for Standardization}},
	institution={ISO},
	number={ISO 10218-1:2025},
	year={2025}
}

@article{huang2022inner,
	title={Inner monologue: Embodied reasoning through planning with language models},
	author={Huang, Wenlong and others},
	journal={arXiv preprint arXiv:2207.05608},
	year={2022}
}

@article{2022_palm_saycan,
	title={Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},
	author={Ahn, Michael and others},
	journal={CoRL},
	year={2022}
}

@article{sermanet2025generating,
	title={Generating Safety Benchmarks for Vision-Language Models in Robotics},
	author={Sermanet, Pierre and others},
	journal={arXiv preprint},
	year={2025}
}